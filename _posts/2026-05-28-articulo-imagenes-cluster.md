---
title:
date:
categories:
tags:
image:
  path:
  lqip:
---

# =================== UC ===================

---

## 1. Introducci√≥n al agrupamiento no supervisado en visi√≥n por computador: contexto hist√≥rico y funcional

### 1.1. Breve historia del an√°lisis de im√°genes sin etiquetas

Durante las d√©cadas de los 80 y 90, la visi√≥n por ordenador se basaba sobre todo en descriptores dise√±ados a mano y en heur√≠sticas bastante espec√≠ficas, modelos como SIFT (Scale-Invariant Feature Transform) y HOG (Histogram of Oriented Gradients) eran clave para capturar detalles locales o patrones generales en las im√°genes.

Estos descriptores dan la posibilidad del tratamiento de tareas tales como el reconocimiento de objetos o clasificaci√≥n, si bien con algunas limitaciones, eran d√©biles contra cambios de iluminaci√≥n, posici√≥n del objeto o escala, lo que los hac√≠a muy sensibles a fallos.

En cuanto al agrupamiento, se basaban anteriormente en m√©tricas simples tales como la distancia euclidiana o la correlaci√≥n directa de estos vectores de caracter√≠sticas, K-means o clustering jer√°rquico (agglomerative clustering) eran algoritmos m√°s populares en ese momento.

Esto cambi√≥ a partir de 2012, con la aparici√≥n del deep learning y las Redes Neuronales Convolucionales (CNNs), que permitieron extraer representaciones mucho m√°s ricas, jer√°rquicas y con un significado m√°s profundo a nivel sem√°ntico.

A medida que los datos no etiquetados comenzaron a superar a los conjuntos anotados manualmente, aparecieron enfoques de aprendizaje no supervisado y auto-supervisado, estas t√©cnicas abrieron nuevas posibilidades para aprovechar al m√°ximo esos enormes vol√∫menes de datos sin necesidad de intervenci√≥n humana directa.

### 1.2. Funcionalidad y retos actuales

El crecimiento exponencial de datos visuales en redes sociales, sistemas m√©dicos, sat√©lites, etc., hace inviable la anotaci√≥n manual.

El agrupamiento no supervisado permite descubrir patrones emergentes, estructurar datos para an√°lisis posteriores y acelerar tareas de clasificaci√≥n o recuperaci√≥n de im√°genes.

**Retos t√©cnicos** incluyen: alta dimensionalidad, escalabilidad, ruido y heterogeneidad en los datos, y evaluaci√≥n sin etiquetas.

---

## 2. Embeddings visuales: definici√≥n, modelos y propiedades matem√°ticas

### 2.1. ¬øPara qu√© y por qu√© se crean los embeddings visuales?

Los **embeddings visuales** se dise√±an para transformar im√°genes en vectores que capturen su **informaci√≥n sem√°ntica** de forma compacta y num√©ricamente manipulable. Este paso es esencial porque los algoritmos tradicionales de aprendizaje autom√°tico y an√°lisis no pueden operar directamente sobre datos de alta dimensi√≥n sin estructura expl√≠cita, como p√≠xeles crudos.

#### Principales objetivos:

- **Reducci√≥n de dimensionalidad**: pasar de una imagen con miles o millones de p√≠xeles a una representaci√≥n en ‚Ñù·µà (por ejemplo, d = 128, 512 o 1024) que concentre la informaci√≥n discriminativa.
- **Comparaci√≥n eficiente**: al representar im√°genes como vectores en un espacio m√©trico, se facilita la medici√≥n de similitud mediante distancias est√°ndar (euclidiana, coseno, etc.).
- **Generalizaci√≥n sem√°ntica**: embeddings bien entrenados agrupan im√°genes con significado similar aunque var√≠en en color, pose, escala, fondo o ruido.
- **Transferencia de conocimiento**: los embeddings pueden ser reutilizados en m√∫ltiples tareas (clasificaci√≥n, b√∫squeda, detecci√≥n, clustering) sin necesidad de reentrenar el modelo base, gracias a su capacidad de abstracci√≥n.
- **Facilitar tareas no supervisadas**: en clustering, recuperaci√≥n de im√°genes o detecci√≥n de anomal√≠as, disponer de una representaci√≥n num√©rica coherente y estructurada es clave para obtener buenos resultados sin etiquetas.

#### Motivaci√≥n t√©cnica:

Los espacios de embedding act√∫an como **espacios latentes** (representaci√≥n matem√°tica comprimida donde los datos complejos (como im√°genes) se transforman en vectores que capturan sus caracter√≠sticas m√°s relevantes [](https://www.ibm.com/think/topics/latent-space)) donde se pueden aplicar t√©cnicas matem√°ticas est√°ndar (√°lgebra lineal, estad√≠stica, geometr√≠a) para modelar relaciones complejas entre im√°genes. En vez de tratar con millones de dimensiones (p√≠xeles), se opera en espacios reducidos pero informativamente ricos.

As√≠, los embeddings permiten **eficiencia computacional**, adem√°s de que **habilitan inteligencia visual**, al capturar conceptos abstractos como "perro", "paisaje urbano" o "estructura anat√≥mica" en vectores que se pueden comparar, agrupar o clasificar.

### 2.2. Qu√© es un embedding visual

Un embedding es una funci√≥n **f: I ‚Üí ‚Ñù·µà** que transforma una imagen **I ‚àà ùìò** en un vector num√©rico en un espacio d-dimensional.

Idealmente, esta funci√≥n debe preservar la similitud sem√°ntica, es decir, im√°genes con contenido similar deben tener embeddings cercanos seg√∫n alguna m√©trica (coseno, euclidiana).

El espacio ‚Ñù·µà suele ser continuo, denso y estructurado, donde la distancia refleja relaciones sem√°nticas.

### 2.3. Arquitecturas para extracci√≥n de embeddings

- **CNN cl√°sicas**: ResNet, DenseNet, EfficientNet, donde las capas finales act√∫an como extractores de caracter√≠sticas globales.
- **Transformers para visi√≥n (ViT)**: procesan la imagen como un conjunto de parches, usando atenci√≥n para modelar relaciones a larga distancia.
- **Modelos multimodales (CLIP)**: entrenados con aprendizaje contrastivo entre im√°genes y texto, generando espacios sem√°nticos alineados entre ambos dominios.
- **Auto-supervisados**: SimCLR, BYOL, DINO que aprenden invariancias sin etiquetas, usando estrategias como la comparaci√≥n entre vistas aumentadas.

### 2.4. Propiedades matem√°ticas relevantes

- **Normalizaci√≥n**: Embeddings suelen ser normalizados (L2 norm) para que la distancia coseno equivalga a la similitud angular.
- **Linealidad y estructuras geom√©tricas**: operaciones vectoriales pueden tener significado sem√°ntico (e.g., interpolaciones).
- **Separabilidad y margen**: modelos buscan maximizar m√°rgenes entre clases.
- **Distribuci√≥n**: la alta dimensionalidad provoca problemas de concentraci√≥n de medida; t√©cnicas de reducci√≥n dimensional buscan mitigarlo.

---

## 3. Proyecci√≥n a espacios de menor dimensi√≥n para an√°lisis y visualizaci√≥n

### 3.1. Motivaciones

- Facilitar la visualizaci√≥n 2D/3D para inspecci√≥n humana.
- Mejorar la eficiencia y calidad del agrupamiento.
- Comprender mejor la estructura latente en el conjunto de datos.

### 3.2. M√©todos cl√°sicos y avanzados

- **PCA**: proyecci√≥n lineal basada en autovalores; r√°pido y determinista.
- **t-SNE**: mantiene estructura local, complejo para datasets grandes.
- **UMAP**: mantiene estructura local y global, escalable, permite inferencia en nuevos datos.
- **Autoencoders y VAE**: codificaciones comprimidas; VAE a√±ade modelo probabil√≠stico generativo.
- **Isomap, LLE**: conservan distancias geod√©sicas y relaciones locales.

### 3.3. Consideraciones pr√°cticas

- Elecci√≥n de n√∫mero de dimensiones (ej. 50 para clustering, 2-3 para visualizaci√≥n).
- Balance entre estructura y eficiencia.
- Manejo de outliers y datos at√≠picos.

---

## 4. Algoritmos de agrupamiento no supervisado: fundamentos y variantes

### 4.1. Clustering basado en particiones: K-Means y variantes

- Minimiza suma de distancias al centroide.
- R√°pido pero requiere definir **k**, asume clusters convexos.
- Variantes: K-Medoids, MiniBatch K-Means.

### 4.2. Clustering jer√°rquico

- Basado en distancias entre puntos o grupos (dendrogramas).
- Agregativo (bottom-up) o divisivo (top-down).
- Escasa escalabilidad, sensible a ruido.

### 4.3. Clustering basado en densidad: DBSCAN y HDBSCAN

- **DBSCAN**: define regiones densas como clusters, detecta ruido.
- **HDBSCAN**: generaliza DBSCAN, maneja densidades variables y jerarqu√≠a.

### 4.4. M√©todos basados en grafos y espectrales

- Construcci√≥n de grafo de similitud (k-NN).
- Usa autovectores de la matriz laplaciana.
- Costoso computacionalmente.

### 4.5. M√©todos modernos y emergentes

- **Deep Clustering**: aprendizaje conjunto de representaci√≥n y clusters (DEC, DeepCluster).
- M√©todos con auto-supervisi√≥n y atenci√≥n.
- Adaptativos a datos en flujo (online/incremental).

---

## 5. Preparaci√≥n y organizaci√≥n de los resultados para interpretaci√≥n y an√°lisis

### 5.1. Etiquetado y asignaci√≥n de grupos

- Asignaci√≥n de etiquetas num√©ricas o categor√≠as.
- Revisi√≥n de clusters peque√±os o dispersos.

### 5.2. Visualizaci√≥n de agrupamientos

- Grids visuales para inspecci√≥n.
- Visualizaci√≥n interactiva (Plotly, Bokeh, Streamlit).
- Mapas de calor y diagramas de distribuci√≥n.

### 5.3. An√°lisis estad√≠stico y m√©tricas

- Tama√±o, distribuci√≥n de clusters.
- M√©tricas: silhouette score, Davies-Bouldin.
- Evaluaci√≥n sin etiquetas: m√©tricas internas.

### 5.4. Uso de resultados para tareas posteriores

- Etiquetado proxy.
- Filtrado y curaci√≥n de datasets.
- Detecci√≥n de anomal√≠as.

---

## 6. Aspectos pr√°cticos y consideraciones t√©cnicas para sistemas a escala

### 6.1. Extracci√≥n eficiente de embeddings

- Uso de GPUs, batch processing.
- Preprocesamiento de im√°genes.
- Paralelizaci√≥n y distribuci√≥n.

### 6.2. Almacenamiento y gesti√≥n de vectores

- Formatos compactos: float16, int8.
- Bases vectoriales: FAISS, Annoy, ScaNN.
- Indexaci√≥n y b√∫squeda de vecinos cercanos.

### 6.3. Escalabilidad en clustering

- Clustering incremental o jer√°rquico distribuido.
- Submuestreo, clustering aproximado.
- Actualizaci√≥n continua.

### 6.4. Evaluaci√≥n y validaci√≥n

- Datasets benchmark.
- Validaci√≥n humana.
- Comparaci√≥n con modelos supervisados.

---

## 7. Aplicaciones reales y casos de uso detallados

### 7.1. Organizaci√≥n y b√∫squeda en fototecas personales y empresariales

- Google Photos, Apple Photos.
- Identificaci√≥n autom√°tica de personas, eventos, lugares.

### 7.2. Curaci√≥n y limpieza de datasets para machine learning

- Eliminaci√≥n de duplicados.
- Balanceo de clases, filtrado de ruido.

### 7.3. An√°lisis en medicina y biolog√≠a

- Agrupamiento en histolog√≠a.
- Pre-clasificaci√≥n en radiolog√≠a sin etiquetas.

### 7.4. Vigilancia satelital y monitoreo ambiental

- Detecci√≥n de cambios en im√°genes terrestres.
- Agrupamiento seg√∫n cobertura o actividad.

### 7.5. Industria y comercio electr√≥nico

- Organizaci√≥n de cat√°logos visuales.
- Recomendaciones por similitud.

---

## 8. Conclusiones y perspectivas futuras

- El aprendizaje no supervisado es clave para escalar el an√°lisis visual.
- La combinaci√≥n embeddings + reducci√≥n dimensional + clustering es una sinergia potente.
- Tendencias futuras: auto-supervisi√≥n, clustering end-to-end, adaptaci√≥n continua.
- Desaf√≠os: balance entre precisi√≥n, interpretabilidad y escalabilidad.
